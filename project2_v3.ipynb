{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62413fe5-2fec-474d-aa1b-cd5f522532a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise to creating a python script to process data and create publication-ready plots\n",
    "# this script re-creates figures 1c and 1d from the paper: \"Single human oocyte transcriptome analysis reveals distinct maturation stage-dependent pathways impacted by age\"\n",
    "\n",
    "# doi: https://doi.org/10.1111/acel.13360\n",
    "# written by: Bella Pfeiffer\n",
    "# written for: CDSBF-550, Boston University Bioinformatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c68f88-e396-4b55-8265-7536ad105844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE  # for dimensionality reduction, similar to Rtsne in R\n",
    "from sklearn.preprocessing import StandardScaler  # like scale() in R\n",
    "from sklearn.decomposition import PCA  # similar to prcomp() in R\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import ttest_ind  # like t.test() in R\n",
    "import matplotlib.pyplot as plt  # plotting library - quite different from ggplot2!\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb6f13-5235-4a25-834e-5fa659fc71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for data processing \n",
    "def load_and_parse_metadata(metadata_file):\n",
    "    \"\"\"\n",
    "    reads and cleans up the metadata from geo. in R, this would typically be done\n",
    "    using readLines() or read.delim(), but python needs a bit more manual parsing\n",
    "    \n",
    "    important note: we're specifically looking for geo-formatted metadata here,\n",
    "    which has those characteristic !Sample_* headers\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # reading the file line by line - in R we might use readLines() instead\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # look for the sample names and maturation stages\n",
    "            # note: in geo files, these have special prefixes\n",
    "            if line.startswith('!Sample_source_name_ch1'):\n",
    "                # strip out those pesky quotes that geo adds\n",
    "                metadata['metadata_id'] = [s.strip('\"') for s in line.strip().split('\\t')[1:]]\n",
    "            elif 'maturation stage:' in line:\n",
    "                # get just the stage name, throwing away the rest\n",
    "                metadata['stage'] = [s.split()[-1].strip('\"') for s in line.strip().split('\\t')[1:]]\n",
    "                break\n",
    "    \n",
    "    # convert to a dataframe - similar to as.data.frame() in R\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    \n",
    "    # let's see what stages we have\n",
    "    print(\"\\nMetadata summary - Stage distribution:\")\n",
    "    print(metadata_df['stage'].value_counts())\n",
    "    return metadata_df\n",
    "\n",
    "def normalize_data_seurat_like(counts_df):\n",
    "    \"\"\"\n",
    "    this implements seurat's normalization approach in python\n",
    "    \n",
    "    key differences from R/seurat:\n",
    "    - we use pandas operations instead of sparse matrices\n",
    "    - log1p is used instead of log() + 1\n",
    "    - the implementation is more explicit here than seurat's internal code\n",
    "    \"\"\"\n",
    "    print(\"normalizing data seurat-style...\")\n",
    "    \n",
    "    # first get library size factors (sum per column)\n",
    "    # in R: colSums(counts_df)\n",
    "    size_factors = counts_df.sum(axis=0)\n",
    "    \n",
    "    # get the scaling factor - in R: median(colSums(counts_df))\n",
    "    median_total = np.median(size_factors)\n",
    "    \n",
    "    # normalize by library size and scale - similar to sweep() in R\n",
    "    normalized = counts_df.div(size_factors) * median_total\n",
    "    \n",
    "    # log transform - in R we'd do log(x + 1)\n",
    "    log_norm = np.log1p(normalized)\n",
    "    return log_norm\n",
    "\n",
    "def calculate_DE_stats(counts_df, stages):\n",
    "    \"\"\"\n",
    "    calculates differential expression between stages\n",
    "    \n",
    "    this is similar to using DESeq2 or edgeR in R, but we're doing a simpler\n",
    "    implementation here with just t-tests. in a real analysis you'd probably\n",
    "    want to use a proper DE package that handles dispersion estimation\n",
    "    \"\"\"\n",
    "    print(\"\\nrunning differential expression analysis...\")\n",
    "    \n",
    "    # clean up any leftover quotes in stage names\n",
    "    stages = np.array([str(s).strip('\"') for s in stages])\n",
    "    \n",
    "    # create masks for our groups - in R we'd use logical indexing\n",
    "    group1_mask = stages == 'GV'\n",
    "    group2_mask = stages == 'MII'\n",
    "    \n",
    "    # doing some sanity checking\n",
    "    print(f\"we have {sum(group1_mask)} GV samples and {sum(group2_mask)} MII samples\")\n",
    "    \n",
    "    # store results for each gene\n",
    "    results = []\n",
    "    for gene in counts_df.index:\n",
    "        # get expression for each group\n",
    "        group1_expr = counts_df.loc[gene, group1_mask]\n",
    "        group2_expr = counts_df.loc[gene, group2_mask]\n",
    "        \n",
    "        # calculate means (adding pseudocount to avoid log(0))\n",
    "        mean1 = np.mean(group1_expr) + 1\n",
    "        mean2 = np.mean(group2_expr) + 1\n",
    "        \n",
    "        # get log2 fold change\n",
    "        log2fc = np.log2(mean2/mean1)\n",
    "        \n",
    "        # run t-test (using welch's t-test by setting equal_var=False)\n",
    "        # this is like t.test(..., var.equal=FALSE) in R\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(group1_expr, group2_expr, equal_var=False)\n",
    "        except:\n",
    "            p_val = 1.0  # if the test fails, be conservative\n",
    "        \n",
    "        # store everything we calculated\n",
    "        results.append({\n",
    "            'gene': gene,\n",
    "            'log2FC': log2fc,\n",
    "            'pvalue': p_val,\n",
    "            '-log10p': -np.log10(p_val) if p_val > 0 else 0,\n",
    "            'mean_GV': np.mean(group1_expr),\n",
    "            'mean_MII': np.mean(group2_expr)\n",
    "        })\n",
    "    \n",
    "    # convert results to dataframe - similar to do.call(rbind, results) in R\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # print some diagnostic info\n",
    "    print(\"\\nquick summary of what we found:\")\n",
    "    print(f\"looked at {len(df_results)} genes total\")\n",
    "    print(\"\\nfold change distribution:\")\n",
    "    print(df_results['log2FC'].describe())\n",
    "    print(\"\\np-value distribution:\")\n",
    "    print(df_results['pvalue'].describe())\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b5970-dfa0-4e2c-b56a-a6e00701ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions specific to plotting\n",
    "def plot_volcano_enhanced(de_stats, output_file='volcano_plot.png'):\n",
    "    \"\"\"\n",
    "    creates a publication-ready volcano plot\n",
    "    \n",
    "    this is trying to replicate something like what you'd get from EnhancedVolcano in R,\n",
    "    but using matplotlib instead of ggplot2. the styling is a bit more manual here!\n",
    "    \"\"\"\n",
    "    # set up the figure - in R/ggplot2 this would be handled automatically\n",
    "    # we're making it wide and short for better proportions\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # thresholds for significance\n",
    "    # note: fold change threshold of 2 means log2FC of 1\n",
    "    fc_threshold = 2  # this would be log2FoldChange in DESeq2\n",
    "    p_threshold = 0.01  # typical threshold for differential expression\n",
    "    \n",
    "    # calculate -log10(p) just once for efficiency\n",
    "    # in R we might do this in the plotting call itself\n",
    "    neg_log_p = -np.log10(de_stats['pvalue'])\n",
    "    \n",
    "    # create masks for different categories of genes\n",
    "    # this is similar to dplyr::mutate() + case_when() in R\n",
    "    signif_up = (de_stats['log2FC'] > np.log2(fc_threshold)) & (de_stats['pvalue'] < p_threshold)\n",
    "    signif_down = (de_stats['log2FC'] < -np.log2(fc_threshold)) & (de_stats['pvalue'] < p_threshold)\n",
    "    fc_only = (abs(de_stats['log2FC']) > np.log2(fc_threshold)) & (de_stats['pvalue'] >= p_threshold)\n",
    "    p_only = (abs(de_stats['log2FC']) <= np.log2(fc_threshold)) & (de_stats['pvalue'] < p_threshold)\n",
    "    \n",
    "    # plot each category with different colors\n",
    "    # in ggplot2 this would be done with a single geom_point() call using aes()\n",
    "    plt.scatter(de_stats.loc[~(signif_up | signif_down | fc_only | p_only), 'log2FC'],\n",
    "               neg_log_p[~(signif_up | signif_down | fc_only | p_only)],\n",
    "               c='gray', alpha=0.5, s=15, label='NS')\n",
    "    \n",
    "    plt.scatter(de_stats.loc[fc_only, 'log2FC'],\n",
    "               neg_log_p[fc_only],\n",
    "               c='green', alpha=0.7, s=15, label='Log₂ FC')\n",
    "    \n",
    "    plt.scatter(de_stats.loc[p_only, 'log2FC'],\n",
    "               neg_log_p[p_only],\n",
    "               c='blue', alpha=0.7, s=15, label='p-value')\n",
    "    \n",
    "    plt.scatter(de_stats.loc[signif_up | signif_down, 'log2FC'],\n",
    "               neg_log_p[signif_up | signif_down],\n",
    "               c='red', alpha=0.7, s=15, label='p-value and log₂ FC')\n",
    "    \n",
    "    # set the axis limits - in ggplot2 this would be scale_x/y_continuous()\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(0, 35)\n",
    "    \n",
    "    # add labels for the most significant genes\n",
    "    # similar to ggrepel in R, but more manual here\n",
    "    significant = signif_up | signif_down\n",
    "    if any(significant):\n",
    "        # find the most interesting genes to label\n",
    "        label_candidates = de_stats[significant].copy()\n",
    "        # combine fold change and p-value to rank genes\n",
    "        label_candidates['score'] = abs(label_candidates['log2FC']) * (-np.log10(label_candidates['pvalue']))\n",
    "        top_genes = label_candidates.nlargest(15, 'score')\n",
    "        \n",
    "        # add text labels with white background for legibility\n",
    "        for idx, row in top_genes.iterrows():\n",
    "            plt.annotate(idx, \n",
    "                        xy=(row['log2FC'], -np.log10(row['pvalue'])),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, color='black',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=0.1))\n",
    "    \n",
    "    # add reference lines - like geom_vline() and geom_hline() in ggplot2\n",
    "    plt.axvline(x=np.log2(fc_threshold), color='gray', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=-np.log2(fc_threshold), color='gray', linestyle='--', alpha=0.3)\n",
    "    plt.axhline(y=-np.log10(p_threshold), color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # label the axes - like labs() in ggplot2\n",
    "    plt.xlabel('avg log₂FC', fontsize=12)\n",
    "    plt.ylabel('-Log₁₀ P', fontsize=12)\n",
    "    \n",
    "    # add some helpful annotations\n",
    "    plt.text(0, 33, '|FC = 2|', ha='center', va='top', color='gray')\n",
    "    plt.text(-4, 33, 'GV markers', ha='center', va='top')\n",
    "    plt.text(4, 33, 'IVM-MII markers', ha='center', va='top')\n",
    "    \n",
    "    # add legend - much more manual than in ggplot2!\n",
    "    plt.legend(frameon=True, fontsize=10, title=None, \n",
    "              bbox_to_anchor=(0.02, 0.98), loc='upper left')\n",
    "    \n",
    "    # make sure the dimensions stick\n",
    "    plt.gcf().set_size_inches(16, 8)\n",
    "    \n",
    "    # save the plot - like ggsave() in R\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def run_tsne_on_pca(pca_result): # not plotting but included here for convenience\n",
    "    \"\"\"\n",
    "    runs t-sne on our pca results for visualization\n",
    "    \n",
    "    this is similar to using Rtsne in R, but scikit-learn's implementation\n",
    "    has some different parameters and defaults\n",
    "    \"\"\"\n",
    "    print(\"\\nrunning t-sne (this might take a minute)...\")\n",
    "    \n",
    "    # create the t-sne object with carefully chosen parameters\n",
    "    tsne = TSNE(\n",
    "        n_components=2,  # we want 2D visualization\n",
    "        # adjust perplexity based on sample size - this is crucial!\n",
    "        perplexity=min(30, pca_result.shape[0] - 1),  \n",
    "        init='pca',  # initialize with pca for better stability\n",
    "        random_state=42,  # for reproducibility\n",
    "        learning_rate='auto',  # let t-sne figure out the best learning rate\n",
    "        n_iter=2000,  # run for longer than default\n",
    "        early_exaggeration=12.0,  # helps with better cluster separation\n",
    "        metric='euclidean'  # standard distance metric\n",
    "    )\n",
    "    \n",
    "    # run t-sne - this is where the magic happens!\n",
    "    tsne_result = tsne.fit_transform(pca_result)\n",
    "    \n",
    "    # print some info about the results\n",
    "    print(f\"t-sne transform done! output shape: {tsne_result.shape}\")\n",
    "    print(\"coordinates range:\", \n",
    "          f\"X: [{tsne_result[:,0].min():.2f}, {tsne_result[:,0].max():.2f}]\",\n",
    "          f\"Y: [{tsne_result[:,1].min():.2f}, {tsne_result[:,1].max():.2f}]\")\n",
    "    \n",
    "    return tsne_result\n",
    "def plot_pca(pca_result, stages, output_file='pca_plot.png'):\n",
    "    \"\"\"\n",
    "    creates a pca plot colored by developmental stage\n",
    "    \n",
    "    this is similar to what you might get from doing:\n",
    "    ggplot(pca_df, aes(PC1, PC2, color=stage)) + geom_point()\n",
    "    but we're using matplotlib's more manual approach\n",
    "    \"\"\"\n",
    "    print(\"\\n=== starting pca plotting routine ===\")\n",
    "    print(f\"working with {pca_result.shape[1]} principal components\")\n",
    "    print(f\"we have {len(stages)} samples to plot\")\n",
    "    \n",
    "    # create a square figure - good for pca plots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # clean up the stage labels - r usually handles this more gracefully\n",
    "    stages = np.array([str(s).strip().strip('\"') for s in stages])\n",
    "    print(f\"found these stages in the data: {np.unique(stages)}\")\n",
    "    \n",
    "    # plot each developmental stage\n",
    "    # we're using specific colors that work well together\n",
    "    # in R/ggplot2 this would be handled by scale_color_manual()\n",
    "    for stage, color in zip(['GV', 'MII'], ['#4444AA', '#FF8C55']):  # nice blue and orange\n",
    "        mask = stages == stage\n",
    "        count = np.sum(mask)\n",
    "        print(f\"\\nplotting {count} samples for {stage} stage\")\n",
    "        \n",
    "        if count > 0:\n",
    "            # print the spread of points - helpful for debugging\n",
    "            print(f\"points spread from:\")\n",
    "            print(f\"   x: {pca_result[mask, 0].min():.2f} to {pca_result[mask, 0].max():.2f}\")\n",
    "            print(f\"   y: {pca_result[mask, 1].min():.2f} to {pca_result[mask, 1].max():.2f}\")\n",
    "            \n",
    "            # create the scatter plot - each stage gets its own color\n",
    "            plt.scatter(pca_result[mask, 0], \n",
    "                       pca_result[mask, 1],\n",
    "                       c=color, \n",
    "                       label=stage, \n",
    "                       alpha=0.9,  # slightly transparent\n",
    "                       s=80,  # nice big points\n",
    "                       edgecolor='white',  # white edges look clean\n",
    "                       linewidth=0.5)\n",
    "    \n",
    "    # label the axes - these are the first two principal components\n",
    "    plt.xlabel('PC1', fontsize=12)\n",
    "    plt.ylabel('PC2', fontsize=12)\n",
    "    plt.title('PCA Plot', fontsize=14)\n",
    "    \n",
    "    # add a light grid - helps with readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # add a legend if we plotted any points\n",
    "    # in ggplot2 this would happen automatically\n",
    "    if len(plt.gca().collections) > 0:\n",
    "        plt.legend(frameon=True, fontsize=12)\n",
    "        print(\"\\nsuccessfully added the legend\")\n",
    "    else:\n",
    "        print(\"\\nwarning: no points were plotted!\")\n",
    "    \n",
    "    # add sample counts in top-left corner\n",
    "    # this would be like annotate() in ggplot2\n",
    "    plt.text(0.02, 0.98,\n",
    "             f\"GV: {sum(stages == 'GV')} samples\\n\"\n",
    "             f\"MII: {sum(stages == 'MII')} samples\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top',\n",
    "             fontsize=10,\n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "    \n",
    "    # make sure everything fits nicely\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save the plot with error handling\n",
    "    try:\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(\"\\nplot saved successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\noops! error saving plot: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def plot_tsne(tsne_result, stages):\n",
    "    \"\"\"\n",
    "    creates a t-sne plot colored by developmental stage\n",
    "    \n",
    "    this is very similar to our pca plot, but using t-sne coordinates\n",
    "    in R this would look almost identical but use different input data\n",
    "    \"\"\"\n",
    "    print(\"\\n=== setting up t-sne visualization ===\")\n",
    "    \n",
    "    # do some sanity checking on our input data\n",
    "    print(\"checking our inputs:\")\n",
    "    print(f\"t-sne coordinates shape: {tsne_result.shape}\")\n",
    "    print(f\"number of samples: {len(stages)}\")\n",
    "    print(f\"first few stages: {stages[:5]}\")\n",
    "    \n",
    "    # clean up stage labels - always good to be thorough!\n",
    "    stages = np.array([str(s).strip().strip('\"') for s in stages])\n",
    "    print(\"\\nafter cleaning, found these stages:\")\n",
    "    unique_counts = pd.Series(stages).value_counts()\n",
    "    print(unique_counts)\n",
    "    \n",
    "    # create a square plot - just like with pca\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # plot each stage with nice colors\n",
    "    # using the same color scheme as pca for consistency\n",
    "    colors = {'GV': '#4444AA', 'MII': '#FF8C55'}\n",
    "    for stage in ['GV', 'MII']:\n",
    "        mask = stages == stage\n",
    "        count = np.sum(mask)\n",
    "        print(f\"\\nworking on {stage} stage:\")\n",
    "        print(f\"found {count} samples\")\n",
    "        \n",
    "        if count > 0:\n",
    "            # check the spread of points\n",
    "            print(f\"points spread from:\")\n",
    "            print(f\"X: [{tsne_result[mask, 0].min():.2f} to {tsne_result[mask, 0].max():.2f}]\")\n",
    "            print(f\"Y: [{tsne_result[mask, 1].min():.2f} to {tsne_result[mask, 1].max():.2f}]\")\n",
    "            \n",
    "            # plot the points for this stage\n",
    "            plt.scatter(tsne_result[mask, 0],\n",
    "                       tsne_result[mask, 1],\n",
    "                       c=colors[stage],\n",
    "                       label=stage,\n",
    "                       alpha=0.8,\n",
    "                       s=100)  # slightly bigger points than pca\n",
    "        else:\n",
    "            print(f\"no samples found for {stage}!\")\n",
    "    \n",
    "    # label everything clearly\n",
    "    plt.xlabel('t-SNE 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE 2', fontsize=12)\n",
    "    plt.title('t-SNE Visualization of Gene Expression', fontsize=14)\n",
    "    \n",
    "    # add a legend if we have data\n",
    "    if len(plt.gca().collections) > 0:\n",
    "        plt.legend(frameon=True, fontsize=12)\n",
    "        print(\"\\nlegend added successfully\")\n",
    "    else:\n",
    "        print(\"\\nwarning: couldn't add legend - no points plotted!\")\n",
    "    \n",
    "    # add sample counts just like in pca plot\n",
    "    plt.text(0.02, 0.98,\n",
    "             f\"GV: {sum(stages == 'GV')} samples\\n\"\n",
    "             f\"MII: {sum(stages == 'MII')} samples\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top',\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save the plot with error checking\n",
    "    try:\n",
    "        plt.savefig('tsne_plot_debug.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\nplot saved successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\noops! error saving plot: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da3fe8-4a4c-45c5-85a9-44e4229e1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data analysis \n",
    "def process_raw_dataset(counts_file, mapping_file, metadata_file):\n",
    "    \"\"\"\n",
    "    loads and combines all our raw data files into a single analysis-ready dataset\n",
    "    \n",
    "    in R/Bioconductor, this would often be handled by functions like:\n",
    "    readRDS() for counts\n",
    "    read.delim() for mapping\n",
    "    and specialized parsers for GEO metadata\n",
    "    \"\"\"\n",
    "    print(\"\\nstarting to load our raw data files...\")\n",
    "    \n",
    "    # let's see where we are and what files we can see\n",
    "    # helpful for debugging path issues!\n",
    "    print(f\"working in: {os.getcwd()}\")\n",
    "    print(f\"i can see these files: {os.listdir()}\")\n",
    "    \n",
    "    # load our three data files\n",
    "    # in R we might use read.delim() for all of these\n",
    "    counts_df = pd.read_csv(counts_file, sep='\\t', index_col=0)  # gene counts\n",
    "    mapping_df = pd.read_csv(mapping_file, sep='\\t', header=None,  # sample mappings\n",
    "                           names=['counts_id', 'metadata_id'])\n",
    "    metadata_df = load_and_parse_metadata(metadata_file)  # sample metadata\n",
    "    \n",
    "    # clean up metadata - R usually handles quotes better!\n",
    "    metadata_df['stage'] = metadata_df['stage'].str.strip('\"')\n",
    "    \n",
    "    # combine metadata with mapping info\n",
    "    # this is like merge() or join() in R\n",
    "    sample_info = pd.merge(mapping_df, metadata_df, on='metadata_id', how='inner')\n",
    "    \n",
    "    # get just the counts we want\n",
    "    # in R: counts_df[, sample_info$counts_id]\n",
    "    processed_df = counts_df[sample_info['counts_id']].copy()\n",
    "    \n",
    "    # extract stages for later use\n",
    "    stages = list(sample_info['stage'])\n",
    "    \n",
    "    # print some useful info about what we found\n",
    "    print(\"\\nlet's see what we have:\")\n",
    "    print(pd.Series(stages).value_counts())\n",
    "    print(\"\\nunique stages:\", np.unique(stages))\n",
    "    \n",
    "    return processed_df, stages\n",
    "\n",
    "def process_data_seurat_like(counts_df, n_pcs=20):\n",
    "    \"\"\"\n",
    "    processes raw counts data using steps similar to seurat\n",
    "    \n",
    "    this replicates the basic seurat workflow:\n",
    "    1. normalize\n",
    "    2. find variable features\n",
    "    3. scale data\n",
    "    4. run pca\n",
    "    \n",
    "    in R/seurat this would be:\n",
    "    NormalizeData() %>%\n",
    "    FindVariableFeatures() %>%\n",
    "    ScaleData() %>%\n",
    "    RunPCA()\n",
    "    \"\"\"\n",
    "    # step 1: normalize the data\n",
    "    normalized_data = normalize_data_seurat_like(counts_df)\n",
    "    \n",
    "    # step 2: find variable features to focus on\n",
    "    var_features = find_variable_features_seurat_like(normalized_data)\n",
    "    \n",
    "    # step 3: scale the variable features\n",
    "    print(\"scaling our variable features...\")\n",
    "    data_to_scale = normalized_data.loc[var_features]\n",
    "    # this is like scale() in R, but handles sparse data better\n",
    "    scaled_data = StandardScaler().fit_transform(data_to_scale.T)\n",
    "    \n",
    "    # step 4: run pca\n",
    "    print(f\"\\nrunning pca, keeping {n_pcs} components...\")\n",
    "    pca = PCA(n_components=n_pcs)\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # see how much variance we captured\n",
    "    var_explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"our {n_pcs} PCs explain {var_explained:.2%} of the variance\")\n",
    "    \n",
    "    return pca_result, var_features, normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313cd85-b2d5-4901-ba75-c777f0fe6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    main analysis workflow - this ties everything together!\n",
    "    \"\"\"\n",
    "    print(\"let's analyze some gene expression data!\")\n",
    "    \n",
    "    # first, load and prep our data\n",
    "    # in real life, you'd want to make these paths configurable\n",
    "    counts_df, stages = process_raw_dataset(\n",
    "        counts_file=\"/Users/bellap/Desktop/cdsbf550/project2_v2/GSE158802_counts.tsv\",\n",
    "        mapping_file=\"/Users/bellap/Desktop/cdsbf550/project2_v2/GSE158802_samples.tsv\",\n",
    "        metadata_file=\"/Users/bellap/Desktop/cdsbf550/project2_v2/GSE158802_series_matrix.txt\"\n",
    "    )\n",
    "    \n",
    "    # let's see what we got\n",
    "    print(\"\\nquick data check:\")\n",
    "    print(f\"our counts matrix is {counts_df.shape}\")\n",
    "    print(f\"we have {len(stages)} samples\")\n",
    "    print(\"samples per stage:\", pd.Series(stages).value_counts())\n",
    "    \n",
    "    # make sure everything is numeric\n",
    "    # in R this would often be automatic\n",
    "    counts_df = counts_df.astype(float)\n",
    "    \n",
    "    # filter out genes with zero counts\n",
    "    # in R: counts_df[rowSums(counts_df) > 0, ]\n",
    "    non_zero_genes = (counts_df.sum(axis=1) > 0)\n",
    "    counts_df = counts_df.loc[non_zero_genes]\n",
    "    print(f\"after filtering, we have {counts_df.shape[0]} genes\")\n",
    "    \n",
    "    # run our seurat-like processing pipeline\n",
    "    print(\"\\nstarting main analysis pipeline...\")\n",
    "    pca_result, var_features, normalized_data = process_data_seurat_like(counts_df)\n",
    "    \n",
    "    # check our pca results\n",
    "    print(\"\\npca output check:\")\n",
    "    print(f\"shape: {pca_result.shape}\")\n",
    "    print(\"first few coordinates:\")\n",
    "    print(pca_result[:5, :2])\n",
    "    \n",
    "    # create all our visualizations\n",
    "    print(\"\\ngenerating plots...\")\n",
    "    plot_pca(pca_result, stages)\n",
    "    \n",
    "    # run t-sne on our pca results\n",
    "    print(\"\\nrunning t-sne for additional visualization...\")\n",
    "    tsne_result = run_tsne_on_pca(pca_result)\n",
    "    plot_tsne(tsne_result, stages)\n",
    "\n",
    "    # finally, run differential expression\n",
    "    print(\"\\nlooking for differentially expressed genes...\")\n",
    "    de_stats = calculate_DE_stats(normalized_data, stages)\n",
    "    plot_volcano_enhanced(de_stats)\n",
    "    \n",
    "    print(\"\\nall done! check the output directory for plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19c985-a86f-464d-90ab-33a50504b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code!\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
